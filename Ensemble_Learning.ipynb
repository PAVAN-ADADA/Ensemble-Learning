{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "\n",
        "Ensemble learning is a machine learning technique that improves predictive performance by combining the outputs of multiple individual models, called \"weak learners,\" into a single, more robust \"strong learner\". The key idea is that the collective wisdom of a group of diverse models, each with its own strengths and weaknesses, can produce more accurate, stable, and generalizable predictions than any single model operating alone\n",
        "\n",
        "Key Idea: Diversity and Aggregation\n",
        "Diversity of Models:\n",
        "The individual models in an ensemble are often different from each other, either through different algorithms, training data subsets, or perspectives.\n",
        "Aggregation:\n",
        "The predictions from these individual models are then combined using a specific strategy, such as:\n",
        "Voting: For classification tasks, models can vote on the most likely class, and the majority vote wins.\n",
        "Averaging or Weighting: For regression or continuous output, predictions can be averaged, or models can be assigned weights based on their performance.\n",
        "Sequential Training (Boosting): Models can be trained one after another, with each new model focusing on correcting the mistakes made by its predecessors.\n",
        "\n",
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Bagging involves training multiple independent models on random, resampled subsets of the data in parallel, focusing on reducing variance and preventing overfitting by aggregating their predictions. Boosting, conversely, trains models sequentially, with each new model focusing on and correcting the errors made by the previous ones, aiming to reduce bias and improve accuracy. Key differences include Bagging's focus on independent models and equal weighting, while Boosting uses dependent models and assigns weights based on performance, making it more suited for high-bias problems\n",
        "\n",
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "\n",
        "Bootstrap sampling is a resampling technique where multiple subsets (bootstrap samples) are created from an original dataset by randomly selecting data points with replacement. This means that a single data point can appear multiple times within a bootstrap sample, while other data points from the original dataset may not be included at all in a particular sample. Each bootstrap sample is typically the same size as the original dataset.\n",
        "In Bagging (Bootstrap Aggregating) methods, such as Random Forest, bootstrap sampling plays a crucial role in promoting diversity among the individual models (e.g., decision trees in a Random Forest).\n",
        "Role in Bagging methods like Random Forest:\n",
        "Creating Diverse Training Sets:\n",
        "Each individual tree in a Random Forest is trained on a different bootstrap sample. This ensures that each tree learns from a slightly different perspective of the data, as some samples are repeated and others are omitted in each bootstrap sample. This diversity helps reduce the correlation between individual trees.\n",
        "Reducing Variance and Preventing Overfitting:\n",
        "By training multiple trees on diverse subsets of the data and then aggregating their predictions (e.g., through majority voting for classification or averaging for regression), Bagging methods effectively reduce the variance of the overall model. This helps to prevent overfitting, as the ensemble model is less sensitive to the specific noise or outliers present in any single training set.\n",
        "Enhancing Model Stability:\n",
        "The aggregation of predictions from multiple models, each trained on a bootstrapped sample, leads to a more stable and robust final prediction compared to a single model trained on the entire dataset. This stability is crucial for generalization to unseen data.\n",
        "\n",
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "Out-of-Bag (OOB) samples are the data points not included in the bootstrap sample used to train a specific tree within an ensemble model like a Random Forest. The OOB score evaluates the model's performance by using these OOB samples as a built-in, unbiased validation set, allowing for an assessment of the model's generalization capabilities without needing a separate validation dataset. The score is calculated by aggregating predictions from all the trees that did not see the OOB sample during their training, providing an estimate of how the model would perform on truly unseen data.\n",
        "\n",
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "\n",
        "Feature importance analysis in a single Decision Tree and a Random Forest both aim to identify the most influential features in a dataset, but they differ in their stability, reliability, and interpretation.\n",
        "Single Decision Tree:\n",
        "Calculation:\n",
        "Feature importance in a single Decision Tree is typically calculated based on the reduction in impurity (e.g., Gini impurity or entropy) achieved by splitting on a particular feature. The higher the impurity reduction, the more important the feature is considered.\n",
        "Stability and Reliability:\n",
        "Feature importance in a single Decision Tree can be unstable and less reliable due to its susceptibility to small changes in the data, leading to different tree structures and potentially varying feature importance rankings. It can also overemphasize features that appear early in the tree, even if other features are also highly predictive.\n",
        "Interpretation:\n",
        "The importance scores are directly linked to the splits made within that specific tree, offering a clear, albeit potentially biased, view of feature influence within that single model.\n",
        "Random Forest:\n",
        "Calculation:\n",
        "Random Forest calculates feature importance by averaging the impurity reduction across all individual Decision Trees in the forest. This ensemble approach provides a more robust and stable measure of importance.\n",
        "Stability and Reliability:\n",
        "Random Forests provide more stable and reliable feature importance scores because they aggregate information from multiple trees, reducing the impact of individual tree variations and overfitting. This helps in identifying genuinely important features.\n",
        "Interpretation:\n",
        "While offering a more reliable global view of feature importance, the aggregated nature of Random Forest importance scores can make direct interpretation of individual feature contributions within a single tree less straightforward. It also needs to be considered that highly correlated features might share importance, potentially making each appear less individually important than they truly are.\n",
        "\n"
      ],
      "metadata": {
        "id": "T_YU_UlgTydJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.'''\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better readability\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort by importance and select top 5\n",
        "top_features = feature_importances.sort_values(by=\"Importance\", ascending=False).head(5)\n",
        "\n",
        "# Print results\n",
        "print(\"Top 5 most important features:\")\n",
        "print(top_features.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY1J0Q1gVfkt",
        "outputId": "2e126706-24e3-43b2-b8a8-08b65569b647"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most important features:\n",
            "             Feature  Importance\n",
            "          worst area    0.139357\n",
            "worst concave points    0.132225\n",
            " mean concave points    0.107046\n",
            "        worst radius    0.082848\n",
            "     worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import sklearn\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Bagging Classifier (handle sklearn version compatibility)\n",
        "if sklearn.__version__ >= \"1.2\":\n",
        "    bagging = BaggingClassifier(\n",
        "        estimator=DecisionTreeClassifier(),\n",
        "        n_estimators=50,\n",
        "        random_state=42\n",
        "    )\n",
        "else:\n",
        "    bagging = BaggingClassifier(\n",
        "        base_estimator=DecisionTreeClassifier(),\n",
        "        n_estimators=50,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "# Print results\n",
        "print(\"scikit-learn version:\", sklearn.__version__)\n",
        "print(\"Accuracy of Single Decision Tree:\", dt_accuracy)\n",
        "print(\"Accuracy of Bagging Classifier:\", bagging_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrZXyBdsVf7A",
        "outputId": "ee60cb7d-1a4a-4a1a-ca4e-3d27efac3aa0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scikit-learn version: 1.6.1\n",
            "Accuracy of Single Decision Tree: 1.0\n",
            "Accuracy of Bagging Classifier: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Iris for demo)\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 3, 5, 7]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Final Accuracy on Test Set:\", final_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhZrjvBkVf-H",
        "outputId": "c031a7e9-4afb-4818-d008-63275d9368c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Accuracy on Test Set: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)'''\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging Regressor (base = Decision Tree)\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "y_pred_bag = bagging_reg.predict(X_test)\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bag)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Results\n",
        "print(\"Mean Squared Error - Bagging Regressor:\", mse_bagging)\n",
        "print(\"Mean Squared Error - Random Forest Regressor:\", mse_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVM7uhKyVgAV",
        "outputId": "cd49f10d-a8c3-46c3-bb74-902b3ec4c7c6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error - Bagging Regressor: 0.25787382250585034\n",
            "Mean Squared Error - Random Forest Regressor: 0.25772464361712627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Simulate Loan Default Dataset\n",
        "# -------------------------------\n",
        "X, y = make_classification(n_samples=5000, n_features=20, n_informative=10,\n",
        "                           n_redundant=5, n_clusters_per_class=2,\n",
        "                           weights=[0.9, 0.1], # imbalanced: 10% defaults\n",
        "                           random_state=42)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Define Models\n",
        "# -------------------------------\n",
        "# Bagging: Random Forest\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=6,            # limit depth to reduce overfitting\n",
        "    max_features=\"sqrt\",    # feature sampling (regularization)\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Boosting: XGBoost\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=4,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,          # regularization\n",
        "    colsample_bytree=0.8,   # feature sampling\n",
        "    eval_metric=\"logloss\",\n",
        "    use_label_encoder=False,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Cross-Validation Evaluation\n",
        "# -------------------------------\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "rf_cv_auc = cross_val_score(rf, X_train, y_train, cv=cv, scoring=\"roc_auc\").mean()\n",
        "xgb_cv_auc = cross_val_score(xgb, X_train, y_train, cv=cv, scoring=\"roc_auc\").mean()\n",
        "\n",
        "print(\"Cross-Validation ROC-AUC Scores:\")\n",
        "print(f\"Random Forest (Bagging): {rf_cv_auc:.4f}\")\n",
        "print(f\"XGBoost (Boosting): {xgb_cv_auc:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "rf.fit(X_train, y_train)\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "\n",
        "rf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\n",
        "xgb_auc = roc_auc_score(y_test, xgb.predict_proba(X_test)[:, 1])\n",
        "\n",
        "print(\"\\nTest Set ROC-AUC:\")\n",
        "print(f\"Random Forest: {rf_auc:.4f}\")\n",
        "print(f\"XGBoost: {xgb_auc:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report (XGBoost):\")\n",
        "print(classification_report(y_test, y_pred_xgb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LDUbFdtVgDc",
        "outputId": "f5307ffe-1817-493b-9ea8-3684bfe3780d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [15:47:42] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [15:47:43] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [15:47:43] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [15:47:43] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [15:47:44] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation ROC-AUC Scores:\n",
            "Random Forest (Bagging): 0.9309\n",
            "XGBoost (Boosting): 0.9566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [15:47:49] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Set ROC-AUC:\n",
            "Random Forest: 0.9131\n",
            "XGBoost: 0.9557\n",
            "\n",
            "Classification Report (XGBoost):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.98      1344\n",
            "           1       0.93      0.65      0.76       156\n",
            "\n",
            "    accuracy                           0.96      1500\n",
            "   macro avg       0.94      0.82      0.87      1500\n",
            "weighted avg       0.96      0.96      0.95      1500\n",
            "\n"
          ]
        }
      ]
    }
  ]
}